{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predictive Autocomplete",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC7VXB6_47WF"
      },
      "source": [
        "## Libraries\n",
        "import wget\n",
        "import torch\n",
        "import xtarfile as tarfile\n",
        "from fairseq.data.data_utils import collate_tokens\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "from transformers import pipeline\n",
        "\n",
        "import sys, time, csv\n",
        "from transformers import RobertaTokenizer, RobertaForMaskedLM\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsnY7gWvtm7Q"
      },
      "source": [
        "# Previous codebase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuBQlrOTy47b"
      },
      "source": [
        "# https://ramsrigoutham.medium.com/sized-fill-in-the-blank-or-multi-mask-filling-with-roberta-and-huggingface-transformers-58eb9e7fb0c\n",
        "def get_predictions(string, tokenizer, model):\n",
        "  token_ids = tokenizer.encode(string, return_tensors='pt')\n",
        "  masked_position = (token_ids.squeeze() == tokenizer.mask_token_id).nonzero()\n",
        "  masked_pos = [mask.item() for mask in masked_position ]\n",
        "\n",
        "  token_ids = token_ids.to('cuda')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    output = model(token_ids)\n",
        "\n",
        "\n",
        "  last_hidden_state = output[0].squeeze()\n",
        "\n",
        "  predictions = []\n",
        "  for index,mask_index in enumerate(masked_pos):\n",
        "    mask_hidden_state = last_hidden_state[mask_index]\n",
        "    idx = torch.topk(mask_hidden_state, k=1, dim=0)[1]\n",
        "    words = [tokenizer.decode(i.item()).strip() for i in idx]\n",
        "    predictions.append(words[0]) # just take the first one since it's the highest confidence\n",
        "    # print ('Mask ', index + 1, 'Guesses : ', words)\n",
        "  \n",
        "  best_guess = ''\n",
        "  for j in predictions:\n",
        "    if j != '':\n",
        "      best_guess = best_guess + ' ' + j[0]\n",
        " \n",
        "  return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heaSLbRqy8gc"
      },
      "source": [
        "def get_mask_indices(string):\n",
        "  mask_indices = []\n",
        "\n",
        "  i = 0\n",
        "  for word in string.split():\n",
        "    if '<mask>' in word:\n",
        "      mask_indices.append(i)\n",
        "    i += 1\n",
        "\n",
        "  return mask_indices\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9fcVkfOzANH"
      },
      "source": [
        "# returns true if strings match minus special characters (we may have some accuracy loss for things like well vs we'll)\n",
        "def strings_match(a, b):\n",
        "  return [c for c in a if c.isalpha()] == [c for c in b if c.isalpha()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoSfppWzzE8p"
      },
      "source": [
        "def current_time_milli():\n",
        "  return round(time.time() * 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8lJUWIazJXZ"
      },
      "source": [
        "def eval(tokenizer, model, masked_dataset, original_dataset):\n",
        "  start_time = current_time_milli()\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for i in range(0, len(masked_dataset)):\n",
        "    row = masked_dataset[i]\n",
        "    if(row == []):\n",
        "      continue\n",
        "\n",
        "    index = row[0]\n",
        "    string = row[1]\n",
        "\n",
        "    # For longer strings, roberta complains with this error: \n",
        "    # Token indices sequence length is longer than the specified maximum sequence length \n",
        "    # for this model (1891 > 512). Running this sequence through the model will result in indexing errors\n",
        "    #\n",
        "    # This is a less-than-ideal workaround for now\n",
        "    if(len(string) > 512):\n",
        "      string = string[:512]\n",
        "\n",
        "    mask_indices = get_mask_indices(string)\n",
        "    predictions = get_predictions(string, tokenizer, model)\n",
        "    total += len(mask_indices)\n",
        "\n",
        "    original_string_tokens = original_dataset[i][1].split()\n",
        "    for j in range(len(mask_indices)):\n",
        "      prediction = predictions[j]\n",
        "      original = original_string_tokens[mask_indices[j]] \n",
        "      if(strings_match(prediction, original)):\n",
        "        correct += 1\n",
        "    \n",
        "    i += 1\n",
        "    \n",
        "    processed_msg = 'Processed message ' + str(i) + ' out of ' + str(len(masked_dataset))\n",
        "    current_time = current_time_milli()\n",
        "    eta_seconds = (current_time - start_time) / 1000 / (i) * (len(masked_dataset) - i)\n",
        "    eta_minutes = int(eta_seconds / 60)\n",
        "    eta_seconds = int(eta_seconds % 60)\n",
        "    eta_msg = 'Estimated time remaining: ' + str(eta_minutes) + ' minutes ' + str(eta_seconds) + ' seconds'\n",
        "    sys.stdout.write('\\r' + processed_msg + ' | ' + eta_msg)\n",
        "    sys.stdout.flush()\n",
        "  \n",
        "  accuracy = float(correct / total)\n",
        "  print('\\n')\n",
        "  print('Model predicted ' + str(correct) + ' out of ' + str(total) + '.')\n",
        "  print('Accuracy: ' + str(accuracy))\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxxqO5QCtus6",
        "outputId": "b0af2f13-b6b7-4580-f38c-0a93bcae8356"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Apr 27 01:48:11 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 466.11       Driver Version: 466.11       CUDA Version: 11.3     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
            "| 40%   30C    P5    27W / 370W |   1349MiB / 10240MiB |     46%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1240    C+G   Insufficient Permissions        N/A      |\n",
            "|    0   N/A  N/A      3696    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
            "|    0   N/A  N/A      4392    C+G   ...ekyb3d8bbwe\\HxOutlook.exe    N/A      |\n",
            "|    0   N/A  N/A      7548    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A      7712    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
            "|    0   N/A  N/A      8152    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
            "|    0   N/A  N/A      8836    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
            "|    0   N/A  N/A      9880    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
            "|    0   N/A  N/A     10792    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A     10988    C+G   Insufficient Permissions        N/A      |\n",
            "|    0   N/A  N/A     11584    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
            "|    0   N/A  N/A     12368    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
            "|    0   N/A  N/A     13712    C+G   ...wekyb3d8bbwe\\Video.UI.exe    N/A      |\n",
            "|    0   N/A  N/A     14652    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
            "|    0   N/A  N/A     15656    C+G   ...ram Files\\LGHUB\\lghub.exe    N/A      |\n",
            "|    0   N/A  N/A     18180    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
            "|    0   N/A  N/A     20324    C+G   ...\\app-1.0.9001\\Discord.exe    N/A      |\n",
            "|    0   N/A  N/A     20388    C+G   ...8wekyb3d8bbwe\\GameBar.exe    N/A      |\n",
            "|    0   N/A  N/A     21648    C+G   ...8bbwe\\WindowsTerminal.exe    N/A      |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HvbWA_a7pBY"
      },
      "source": [
        "## Filling masks (example)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdYqpVB7691R",
        "outputId": "ffea0219-5818-476b-b024-2bfd164397ea"
      },
      "source": [
        "# Initialize MLM pipeline\n",
        "mlm = pipeline('fill-mask')\n",
        "\n",
        "# Get mask token\n",
        "mask = mlm.tokenizer.mask_token\n",
        "\n",
        "# Get result for particular masked phrase\n",
        "results = mlm('Read the rest of this <mask> to understand things in more detail', topk=5)\n",
        "\n",
        "# Print result\n",
        "for result in results:\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sequence': 'Read the rest of this article to understand things in more detail', 'score': 0.35419148206710815, 'token': 1566, 'token_str': ' article'}\n",
            "{'sequence': 'Read the rest of this post to understand things in more detail', 'score': 0.20478709042072296, 'token': 618, 'token_str': ' post'}\n",
            "{'sequence': 'Read the rest of this guide to understand things in more detail', 'score': 0.07164707034826279, 'token': 4704, 'token_str': ' guide'}\n",
            "{'sequence': 'Read the rest of this essay to understand things in more detail', 'score': 0.06781881302595139, 'token': 14700, 'token_str': ' essay'}\n",
            "{'sequence': 'Read the rest of this blog to understand things in more detail', 'score': 0.04165174812078476, 'token': 5059, 'token_str': ' blog'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68Nbg4wmF19r"
      },
      "source": [
        "# Testing with different models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbB6qIstF-bu",
        "outputId": "569b419f-a076-4aec-a7e4-985ee118a45b"
      },
      "source": [
        "# List available models\n",
        "torch.hub.list('pytorch/fairseq')  # [..., 'transformer_lm.wmt19.en', ...]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\Juan Parra/.cache\\torch\\hub\\pytorch_fairseq_master\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bart.base',\n",
              " 'bart.large',\n",
              " 'bart.large.cnn',\n",
              " 'bart.large.mnli',\n",
              " 'bart.large.xsum',\n",
              " 'bpe',\n",
              " 'camembert',\n",
              " 'camembert-base',\n",
              " 'camembert-base-ccnet',\n",
              " 'camembert-base-ccnet-4gb',\n",
              " 'camembert-base-oscar-4gb',\n",
              " 'camembert-base-wikipedia-4gb',\n",
              " 'camembert-large',\n",
              " 'camembert.v0',\n",
              " 'conv.stories',\n",
              " 'conv.stories.pretrained',\n",
              " 'conv.wmt14.en-de',\n",
              " 'conv.wmt14.en-fr',\n",
              " 'conv.wmt17.en-de',\n",
              " 'data.stories',\n",
              " 'dynamicconv.glu.wmt14.en-fr',\n",
              " 'dynamicconv.glu.wmt16.en-de',\n",
              " 'dynamicconv.glu.wmt17.en-de',\n",
              " 'dynamicconv.glu.wmt17.zh-en',\n",
              " 'dynamicconv.no_glu.iwslt14.de-en',\n",
              " 'dynamicconv.no_glu.wmt16.en-de',\n",
              " 'lightconv.glu.wmt14.en-fr',\n",
              " 'lightconv.glu.wmt16.en-de',\n",
              " 'lightconv.glu.wmt17.en-de',\n",
              " 'lightconv.glu.wmt17.zh-en',\n",
              " 'lightconv.no_glu.iwslt14.de-en',\n",
              " 'lightconv.no_glu.wmt16.en-de',\n",
              " 'roberta.base',\n",
              " 'roberta.large',\n",
              " 'roberta.large.mnli',\n",
              " 'roberta.large.wsc',\n",
              " 'tokenizer',\n",
              " 'transformer.wmt14.en-fr',\n",
              " 'transformer.wmt16.en-de',\n",
              " 'transformer.wmt18.en-de',\n",
              " 'transformer.wmt19.de-en',\n",
              " 'transformer.wmt19.de-en.single_model',\n",
              " 'transformer.wmt19.en-de',\n",
              " 'transformer.wmt19.en-de.single_model',\n",
              " 'transformer.wmt19.en-ru',\n",
              " 'transformer.wmt19.en-ru.single_model',\n",
              " 'transformer.wmt19.ru-en',\n",
              " 'transformer.wmt19.ru-en.single_model',\n",
              " 'transformer_lm.gbw.adaptive_huge',\n",
              " 'transformer_lm.wiki103.adaptive',\n",
              " 'transformer_lm.wmt19.de',\n",
              " 'transformer_lm.wmt19.en',\n",
              " 'transformer_lm.wmt19.ru',\n",
              " 'xlmr.base',\n",
              " 'xlmr.large']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8Xc_TXdBkO_"
      },
      "source": [
        "## Different pre-trained model results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpHideOBGIm8"
      },
      "source": [
        "import sys, time, csv\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
        "\n",
        "def main():\n",
        "  models = ['roberta-base', 'roberta-large', 'distilroberta-base']\n",
        "  best_model = ''\n",
        "  accuracy = 0.0;\n",
        "  for entry in models:\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(entry)\n",
        "    model = RobertaForMaskedLM.from_pretrained(entry)\n",
        "    model.eval()\n",
        "\n",
        "    # move to cuda if available\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "      model = model.to('cuda')\n",
        "      logging.info(\"Using cuda now\")\n",
        "\n",
        "    with open('project_dataset/test_masked.csv', newline='') as file:\n",
        "      masked_dataset = list(csv.reader(file))\n",
        "\n",
        "    with open('project_dataset/test.csv', newline='') as file:\n",
        "      original_dataset = list(csv.reader(file))\n",
        "    \n",
        "    # remove headers\n",
        "    masked_dataset.pop(0)\n",
        "    original_dataset.pop(0)\n",
        "    model_accuracy = eval(tokenizer, model, masked_dataset, original_dataset)\n",
        "    if model_accuracy > accuracy:\n",
        "      best_model = model\n",
        "      accuracy = model_accuracy\n",
        "\n",
        "max_int = sys.maxsize\n",
        "while True:\n",
        "    # decrease the maxInt value by factor 10 \n",
        "    # as long as the OverflowError occurs.\n",
        "\n",
        "    try:\n",
        "        csv.field_size_limit(max_int)\n",
        "        break\n",
        "    except OverflowError:\n",
        "        max_int = int(max_int/10)\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhBiIOa7Born"
      },
      "source": [
        "## Fine Tuning Roberta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KykmgDjrE0b7"
      },
      "source": [
        "# Model paths\n",
        "MODEL_TYPE = \"roberta\" \n",
        "MODEL_DIR = \"models/roberta\" \n",
        "OUTPUT_DIR = \"models/roberta/output\" \n",
        "TRAIN_PATH = \"data/train.txt\" \n",
        "EVAL_PATH = \"data/dev.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8RslYv41t6m"
      },
      "source": [
        "cmd = \"\"\"\n",
        "    TOTAL_NUM_UPDATES=2036  # 10 epochs through RTE for bsz 16\n",
        "    WARMUP_UPDATES=122      # 6 percent of the number of updates\n",
        "    LR=2e-05                # Peak LR for polynomial LR scheduler.\n",
        "    NUM_CLASSES=2\n",
        "    MAX_SENTENCES=16        # Batch size.\n",
        "    ROBERTA_PATH=roberta.large/model.pt\n",
        "\n",
        "    CUDA_VISIBLE_DEVICES=0 fairseq-train RTE-bin/ \\\n",
        "    --restore-file $ROBERTA_PATH \\\n",
        "    --max-positions 512 \\\n",
        "    --batch-size $MAX_SENTENCES \\\n",
        "    --max-tokens 4400 \\\n",
        "    --task sentence_prediction \\\n",
        "    --reset-optimizer --reset-dataloader --reset-meters \\\n",
        "    --required-batch-size-multiple 1 \\\n",
        "    --init-token 0 --separator-token 2 \\\n",
        "    --arch roberta_large \\\n",
        "    --criterion sentence_prediction \\\n",
        "    --num-classes $NUM_CLASSES \\\n",
        "    --dropout 0.1 --attention-dropout 0.1 \\\n",
        "    --weight-decay 0.1 --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 \\\n",
        "    --clip-norm 0.0 \\\n",
        "    --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES --warmup-updates $WARMUP_UPDATES \\\n",
        "    --fp16 --fp16-init-scale 4 --threshold-loss-scale 1 --fp16-scale-window 128 \\\n",
        "    --max-epoch 10 \\\n",
        "    --find-unused-parameters \\\n",
        "    --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric; \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "oDXUEoAdm4Hs",
        "outputId": "5532641f-bbf3-40c6-924c-06a7f290c53f"
      },
      "source": [
        "import sys, time, csv\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
        "\n",
        "def main():\n",
        "  models = ['tmp']\n",
        "  best_model = ''\n",
        "  accuracy = 0.0;\n",
        "  for entry in models:\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)\n",
        "    model = RobertaClass()\n",
        "    model = RobertaForMaskedLM.from_pretrained(entry)\n",
        "    # optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "    # loss_function = torch.nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "\n",
        "    # move to cuda if available\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "      model = model.to('cuda')\n",
        "      logging.info(\"Using cuda now\")\n",
        "\n",
        "    with open('project_dataset/test_masked.csv', newline='') as file:\n",
        "      masked_dataset = list(csv.reader(file))\n",
        "\n",
        "    with open('project_dataset/test.csv', newline='') as file:\n",
        "      original_dataset = list(csv.reader(file))\n",
        "    \n",
        "    # remove headers\n",
        "    masked_dataset.pop(0)\n",
        "    original_dataset.pop(0)\n",
        "    model_accuracy = eval(tokenizer, model, masked_dataset, original_dataset)\n",
        "    if model_accuracy > accuracy:\n",
        "      best_model = model\n",
        "      accuracy = model_accuracy\n",
        "\n",
        "max_int = sys.maxsize\n",
        "while True:\n",
        "    # decrease the maxInt value by factor 10 \n",
        "    # as long as the OverflowError occurs.\n",
        "\n",
        "    try:\n",
        "        csv.field_size_limit(max_int)\n",
        "        break\n",
        "    except OverflowError:\n",
        "        max_int = int(max_int/10)\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-9-488b1048967f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOverflowError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mmax_int\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_int\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-9-488b1048967f>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'roberta-base'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobertaClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobertaForMaskedLM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'RobertaClass' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhpDjB5_CVo6"
      },
      "source": [
        "# Updated accuracy based on fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ynul-XCmCVGg"
      },
      "source": [
        "from fairseq.models.roberta import RobertaModel\n",
        "\n",
        "roberta = RobertaModel.from_pretrained(\n",
        "    'checkpoints/',\n",
        "    checkpoint_file='checkpoint_best.pt',\n",
        "    data_name_or_path='RTE-bin'\n",
        ")\n",
        "\n",
        "label_fn = lambda label: roberta.task.label_dictionary.string(\n",
        "    [label + roberta.task.label_dictionary.nspecial]\n",
        ")\n",
        "ncorrect, nsamples = 0, 0\n",
        "roberta.cuda()\n",
        "roberta.eval()\n",
        "with open('glue_data/RTE/dev.tsv') as fin:\n",
        "    fin.readline()\n",
        "    for index, line in enumerate(fin):\n",
        "        tokens = line.strip().split('\\t')\n",
        "        sent1, sent2, target = tokens[1], tokens[2], tokens[3]\n",
        "        tokens = roberta.encode(sent1, sent2)\n",
        "        prediction = roberta.predict('sentence_classification_head', tokens).argmax().item()\n",
        "        prediction_label = label_fn(prediction)\n",
        "        ncorrect += int(prediction_label == target)\n",
        "        nsamples += 1\n",
        "print('| Accuracy: ', float(ncorrect)/float(nsamples))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}