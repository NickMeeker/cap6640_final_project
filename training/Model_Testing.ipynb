{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predictive Autocomplete",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC7VXB6_47WF"
      },
      "source": [
        "## Libraries\n",
        "import wget\n",
        "import torch\n",
        "import xtarfile as tarfile\n",
        "from fairseq.data.data_utils import collate_tokens\n",
        "from fairseq.models.roberta import RobertaModel\n",
        "from transformers import pipeline\n",
        "\n",
        "import sys, time, csv\n",
        "from transformers import RobertaTokenizer, RobertaForMaskedLM\n"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JsnY7gWvtm7Q"
      },
      "source": [
        "# Previous codebase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuBQlrOTy47b"
      },
      "source": [
        "# https://ramsrigoutham.medium.com/sized-fill-in-the-blank-or-multi-mask-filling-with-roberta-and-huggingface-transformers-58eb9e7fb0c\n",
        "def get_predictions(string, tokenizer, model):\n",
        "  token_ids = tokenizer.encode(string, return_tensors='pt')\n",
        "  masked_position = (token_ids.squeeze() == tokenizer.mask_token_id).nonzero()\n",
        "  masked_pos = [mask.item() for mask in masked_position ]\n",
        "\n",
        "  token_ids = token_ids.to('cuda')\n",
        "\n",
        "  with torch.no_grad():\n",
        "    output = model(token_ids)\n",
        "\n",
        "\n",
        "  last_hidden_state = output[0].squeeze()\n",
        "\n",
        "  predictions = []\n",
        "  for index,mask_index in enumerate(masked_pos):\n",
        "    mask_hidden_state = last_hidden_state[mask_index]\n",
        "    idx = torch.topk(mask_hidden_state, k=1, dim=0)[1]\n",
        "    words = [tokenizer.decode(i.item()).strip() for i in idx]\n",
        "    predictions.append(words[0]) # just take the first one since it's the highest confidence\n",
        "    # print ('Mask ', index + 1, 'Guesses : ', words)\n",
        "  \n",
        "  best_guess = ''\n",
        "  for j in predictions:\n",
        "    if j != '':\n",
        "      best_guess = best_guess + ' ' + j[0]\n",
        " \n",
        "  return predictions"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heaSLbRqy8gc"
      },
      "source": [
        "def get_mask_indices(string):\n",
        "  mask_indices = []\n",
        "\n",
        "  i = 0\n",
        "  for word in string.split():\n",
        "    if '<mask>' in word:\n",
        "      mask_indices.append(i)\n",
        "    i += 1\n",
        "\n",
        "  return mask_indices\n"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9fcVkfOzANH"
      },
      "source": [
        "# returns true if strings match minus special characters (we may have some accuracy loss for things like well vs we'll)\n",
        "def strings_match(a, b):\n",
        "  return [c for c in a if c.isalpha()] == [c for c in b if c.isalpha()]"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoSfppWzzE8p"
      },
      "source": [
        "def current_time_milli():\n",
        "  return round(time.time() * 1000)"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8lJUWIazJXZ"
      },
      "source": [
        "def eval(tokenizer, model, masked_dataset, original_dataset):\n",
        "  start_time = current_time_milli()\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for i in range(0, len(masked_dataset)):\n",
        "    row = masked_dataset[i]\n",
        "    if(row == []):\n",
        "      continue\n",
        "\n",
        "    index = row[0]\n",
        "    string = row[1]\n",
        "\n",
        "    # For longer strings, roberta complains with this error: \n",
        "    # Token indices sequence length is longer than the specified maximum sequence length \n",
        "    # for this model (1891 > 512). Running this sequence through the model will result in indexing errors\n",
        "    #\n",
        "    # This is a less-than-ideal workaround for now\n",
        "    if(len(string) > 512):\n",
        "      string = string[:512]\n",
        "\n",
        "    mask_indices = get_mask_indices(string)\n",
        "    predictions = get_predictions(string, tokenizer, model)\n",
        "    total += len(mask_indices)\n",
        "\n",
        "    original_string_tokens = original_dataset[i][1].split()\n",
        "    for j in range(len(mask_indices)):\n",
        "      prediction = predictions[j]\n",
        "      original = original_string_tokens[mask_indices[j]] \n",
        "      if(strings_match(prediction, original)):\n",
        "        correct += 1\n",
        "    \n",
        "    i += 1\n",
        "    \n",
        "    processed_msg = 'Processed message ' + str(i) + ' out of ' + str(len(masked_dataset))\n",
        "    current_time = current_time_milli()\n",
        "    eta_seconds = (current_time - start_time) / 1000 / (i) * (len(masked_dataset) - i)\n",
        "    eta_minutes = int(eta_seconds / 60)\n",
        "    eta_seconds = int(eta_seconds % 60)\n",
        "    eta_msg = 'Estimated time remaining: ' + str(eta_minutes) + ' minutes ' + str(eta_seconds) + ' seconds'\n",
        "    sys.stdout.write('\\r' + processed_msg + ' | ' + eta_msg)\n",
        "    sys.stdout.flush()\n",
        "  \n",
        "  accuracy = float(correct / total)\n",
        "  print('\\n')\n",
        "  print('Model predicted ' + str(correct) + ' out of ' + str(total) + '.')\n",
        "  print('Accuracy: ' + str(accuracy))\n",
        "\n",
        "  return accuracy"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxxqO5QCtus6",
        "outputId": "caeb0871-e2b6-417d-b7e1-d8a4baa0f88d"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Apr 29 19:51:02 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 466.11       Driver Version: 466.11       CUDA Version: 11.3     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name            TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA GeForce ... WDDM  | 00000000:01:00.0  On |                  N/A |\n",
            "| 43%   32C    P5    26W / 370W |   1632MiB / 10240MiB |     45%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|    0   N/A  N/A      1240    C+G   Insufficient Permissions        N/A      |\n",
            "|    0   N/A  N/A      1848    C+G   ...icrosoft VS Code\\Code.exe    N/A      |\n",
            "|    0   N/A  N/A      3696    C+G   ...nputApp\\TextInputHost.exe    N/A      |\n",
            "|    0   N/A  N/A      4008    C+G   ...ekyb3d8bbwe\\YourPhone.exe    N/A      |\n",
            "|    0   N/A  N/A      7548    C+G   ...y\\ShellExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A      8152    C+G   C:\\Windows\\explorer.exe         N/A      |\n",
            "|    0   N/A  N/A      8836    C+G   ...lPanel\\SystemSettings.exe    N/A      |\n",
            "|    0   N/A  N/A      9880    C+G   ...5n1h2txyewy\\SearchApp.exe    N/A      |\n",
            "|    0   N/A  N/A     10792    C+G   ...artMenuExperienceHost.exe    N/A      |\n",
            "|    0   N/A  N/A     10988    C+G   Insufficient Permissions        N/A      |\n",
            "|    0   N/A  N/A     11232    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
            "|    0   N/A  N/A     11584    C+G   ...perience\\NVIDIA Share.exe    N/A      |\n",
            "|    0   N/A  N/A     14652    C+G   ...in7x64\\steamwebhelper.exe    N/A      |\n",
            "|    0   N/A  N/A     15656    C+G   ...ram Files\\LGHUB\\lghub.exe    N/A      |\n",
            "|    0   N/A  N/A     18180    C+G   ...ge\\Application\\msedge.exe    N/A      |\n",
            "|    0   N/A  N/A     20324    C+G   ...\\app-1.0.9001\\Discord.exe    N/A      |\n",
            "|    0   N/A  N/A     21280    C+G   ...zpdnekdrzrea0\\Spotify.exe    N/A      |\n",
            "|    0   N/A  N/A     21648    C+G   ...8bbwe\\WindowsTerminal.exe    N/A      |\n",
            "|    0   N/A  N/A     27784    C+G   ...Roaming\\Zoom\\bin\\Zoom.exe    N/A      |\n",
            "|    0   N/A  N/A     29668    C+G   ...bbwe\\Microsoft.Photos.exe    N/A      |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HvbWA_a7pBY"
      },
      "source": [
        "## Filling masks (example)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdYqpVB7691R",
        "outputId": "08de38ba-3a01-46b1-ba5c-b4c3a969c9a1"
      },
      "source": [
        "# Initialize MLM pipeline\n",
        "mlm = pipeline('fill-mask')\n",
        "\n",
        "# Get mask token\n",
        "mask = mlm.tokenizer.mask_token\n",
        "\n",
        "# Get result for particular masked phrase\n",
        "results = mlm('Read the rest of this <mask> to understand things in more detail', topk=5)\n",
        "\n",
        "# Print result\n",
        "for result in results:\n",
        "  print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sequence': 'Read the rest of this article to understand things in more detail', 'score': 0.35419148206710815, 'token': 1566, 'token_str': ' article'}\n",
            "{'sequence': 'Read the rest of this post to understand things in more detail', 'score': 0.20478709042072296, 'token': 618, 'token_str': ' post'}\n",
            "{'sequence': 'Read the rest of this guide to understand things in more detail', 'score': 0.07164707034826279, 'token': 4704, 'token_str': ' guide'}\n",
            "{'sequence': 'Read the rest of this essay to understand things in more detail', 'score': 0.06781881302595139, 'token': 14700, 'token_str': ' essay'}\n",
            "{'sequence': 'Read the rest of this blog to understand things in more detail', 'score': 0.04165174812078476, 'token': 5059, 'token_str': ' blog'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68Nbg4wmF19r"
      },
      "source": [
        "# Testing with different models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbB6qIstF-bu",
        "outputId": "27b49d15-1238-453b-e3d2-8ed4d7663b01"
      },
      "source": [
        "# List available models\n",
        "torch.hub.list('pytorch/fairseq')  # [..., 'transformer_lm.wmt19.en', ...]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\Juan Parra/.cache\\torch\\hub\\pytorch_fairseq_master\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bart.base',\n",
              " 'bart.large',\n",
              " 'bart.large.cnn',\n",
              " 'bart.large.mnli',\n",
              " 'bart.large.xsum',\n",
              " 'bpe',\n",
              " 'camembert',\n",
              " 'camembert-base',\n",
              " 'camembert-base-ccnet',\n",
              " 'camembert-base-ccnet-4gb',\n",
              " 'camembert-base-oscar-4gb',\n",
              " 'camembert-base-wikipedia-4gb',\n",
              " 'camembert-large',\n",
              " 'camembert.v0',\n",
              " 'conv.stories',\n",
              " 'conv.stories.pretrained',\n",
              " 'conv.wmt14.en-de',\n",
              " 'conv.wmt14.en-fr',\n",
              " 'conv.wmt17.en-de',\n",
              " 'data.stories',\n",
              " 'dynamicconv.glu.wmt14.en-fr',\n",
              " 'dynamicconv.glu.wmt16.en-de',\n",
              " 'dynamicconv.glu.wmt17.en-de',\n",
              " 'dynamicconv.glu.wmt17.zh-en',\n",
              " 'dynamicconv.no_glu.iwslt14.de-en',\n",
              " 'dynamicconv.no_glu.wmt16.en-de',\n",
              " 'lightconv.glu.wmt14.en-fr',\n",
              " 'lightconv.glu.wmt16.en-de',\n",
              " 'lightconv.glu.wmt17.en-de',\n",
              " 'lightconv.glu.wmt17.zh-en',\n",
              " 'lightconv.no_glu.iwslt14.de-en',\n",
              " 'lightconv.no_glu.wmt16.en-de',\n",
              " 'roberta.base',\n",
              " 'roberta.large',\n",
              " 'roberta.large.mnli',\n",
              " 'roberta.large.wsc',\n",
              " 'tokenizer',\n",
              " 'transformer.wmt14.en-fr',\n",
              " 'transformer.wmt16.en-de',\n",
              " 'transformer.wmt18.en-de',\n",
              " 'transformer.wmt19.de-en',\n",
              " 'transformer.wmt19.de-en.single_model',\n",
              " 'transformer.wmt19.en-de',\n",
              " 'transformer.wmt19.en-de.single_model',\n",
              " 'transformer.wmt19.en-ru',\n",
              " 'transformer.wmt19.en-ru.single_model',\n",
              " 'transformer.wmt19.ru-en',\n",
              " 'transformer.wmt19.ru-en.single_model',\n",
              " 'transformer_lm.gbw.adaptive_huge',\n",
              " 'transformer_lm.wiki103.adaptive',\n",
              " 'transformer_lm.wmt19.de',\n",
              " 'transformer_lm.wmt19.en',\n",
              " 'transformer_lm.wmt19.ru',\n",
              " 'xlmr.base',\n",
              " 'xlmr.large']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8Xc_TXdBkO_"
      },
      "source": [
        "## Different pre-trained model results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpHideOBGIm8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3d4b2ce3-ac6c-49a8-d3e5-36f1a2e022f9"
      },
      "source": [
        "import sys, time, csv\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
        "\n",
        "def main():\n",
        "  models = ['roberta-base', 'roberta-large', 'distilroberta-base']\n",
        "  best_model = ''\n",
        "  accuracy = 0.0;\n",
        "  for entry in models:\n",
        "    tokenizer = RobertaTokenizer.from_pretrained(entry)\n",
        "    model = RobertaForMaskedLM.from_pretrained(entry)\n",
        "    model.eval()\n",
        "\n",
        "    # move to cuda if available\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "      model = model.to('cuda')\n",
        "      logging.info(\"Using cuda now\")\n",
        "\n",
        "    with open('project_dataset/test_masked.csv', newline='') as file:\n",
        "      masked_dataset = list(csv.reader(file))\n",
        "\n",
        "    with open('project_dataset/test.csv', newline='') as file:\n",
        "      original_dataset = list(csv.reader(file))\n",
        "    \n",
        "    # remove headers\n",
        "    masked_dataset.pop(0)\n",
        "    original_dataset.pop(0)\n",
        "    model_accuracy = eval(tokenizer, model, masked_dataset, original_dataset)\n",
        "    if model_accuracy > accuracy:\n",
        "      best_model = model\n",
        "      accuracy = model_accuracy\n",
        "\n",
        "max_int = sys.maxsize\n",
        "while True:\n",
        "    # decrease the maxInt value by factor 10 \n",
        "    # as long as the OverflowError occurs.\n",
        "\n",
        "    try:\n",
        "        csv.field_size_limit(max_int)\n",
        "        break\n",
        "    except OverflowError:\n",
        "        max_int = int(max_int/10)\n",
        "main()"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-29 22:11:59 | INFO | root | Using cuda now\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Processed message 86 out of 206961 | Estimated time remaining: 28 minutes 47 seconds"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-86-6ae93b2bd9ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOverflowError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mmax_int\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_int\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-86-6ae93b2bd9ea>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mmasked_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0moriginal_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mmodel_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasked_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmodel_accuracy\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m       \u001b[0mbest_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-80-a3750e900055>\u001b[0m in \u001b[0;36meval\u001b[1;34m(tokenizer, model, masked_dataset, original_dataset)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mmask_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_mask_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-82-52cad5b5d251>\u001b[0m in \u001b[0;36mget_predictions\u001b[1;34m(string, tokenizer, model)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\juan parra\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\juan parra\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1047\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1049\u001b[1;33m         outputs = self.roberta(\n\u001b[0m\u001b[0;32m   1050\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\juan parra\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\juan parra\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    813\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m         )\n\u001b[1;32m--> 815\u001b[1;33m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[0;32m    816\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\juan parra\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\juan parra\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    506\u001b[0m                 )\n\u001b[0;32m    507\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 508\u001b[1;33m                 layer_outputs = layer_module(\n\u001b[0m\u001b[0;32m    509\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\juan parra\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\juan parra\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    393\u001b[0m         \u001b[1;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 395\u001b[1;33m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[0;32m    396\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    397\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\juan parra\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\juan parra\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     ):\n\u001b[1;32m--> 323\u001b[1;33m         self_outputs = self.self(\n\u001b[0m\u001b[0;32m    324\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\juan parra\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\juan parra\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     ):\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mmixed_query_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;31m# If this is instantiated as a cross-attention module, the keys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\juan parra\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\juan parra\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\users\\juan parra\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1751\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1752\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1753\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRBbIuKXUvK7"
      },
      "source": [
        "import happytransformer\n",
        "from happytransformer import HappyWordPrediction"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otcF7DIiYDu3",
        "outputId": "bb2dc2ea-ae19-4a43-c6fe-71cc22774f40"
      },
      "source": [
        "happy_roberta = HappyWordPrediction(\"ROBERTA\", \"roberta-base\")\n",
        "result = happy_roberta.predict_mask(\"Read the rest of this [MASK] to understand things in more detail\", top_k=5)\n",
        "print(result)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-29 21:38:41 | INFO | happytransformer.happy_transformer | Using model: cuda\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[WordPredictionResult(token=' article', score=0.48412269353866577), WordPredictionResult(token=' post', score=0.3602624535560608), WordPredictionResult(token=' piece', score=0.023724298924207687), WordPredictionResult(token=' paper', score=0.01611217111349106), WordPredictionResult(token=' story', score=0.015522404573857784)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4GTjkAOYAcm",
        "outputId": "323c58de-b2bc-4f3e-9aab-4481e3088147"
      },
      "source": [
        "happy_bert = HappyWordPrediction(\"BERT\", \"bert-base-uncased\")\n",
        "result = happy_bert.predict_mask(\"To better the world I would invest in [MASK] and education.\", top_k=2)\n",
        "print(result)"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "2021-04-29 21:24:55 | INFO | happytransformer.happy_transformer | Using model: cuda\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[WordPredictionResult(token='health', score=0.22784867882728577), WordPredictionResult(token='research', score=0.17031845450401306)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-M2gbD3V-eU",
        "outputId": "bbbf13ff-7133-4275-d3dc-b0a371445d1e"
      },
      "source": [
        "happy_albert = HappyWordPrediction(\"ALBERT\", \"albert-xxlarge-v2\")\n",
        "result = happy_albert.predict_mask(\"To better the world I would invest in [MASK] and education.\", top_k=2)\n",
        "print(result)"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-29 21:25:00 | INFO | happytransformer.happy_transformer | Using model: cuda\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[WordPredictionResult(token='infrastructure', score=0.09300383180379868), WordPredictionResult(token='healthcare', score=0.07224401831626892)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhBiIOa7Born"
      },
      "source": [
        "## Fine Tuning Roberta"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KykmgDjrE0b7"
      },
      "source": [
        "# Model paths\n",
        "MODEL_TYPE = \"roberta\" \n",
        "MODEL_DIR = \"models/roberta\" \n",
        "OUTPUT_DIR = \"models/roberta/output\" \n",
        "TRAIN_PATH = \"data/train.txt\" \n",
        "EVAL_PATH = \"data/dev.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8RslYv41t6m"
      },
      "source": [
        "cmd = \"\"\"\n",
        "    TOTAL_NUM_UPDATES=2036  # 10 epochs through RTE for bsz 16\n",
        "    WARMUP_UPDATES=122      # 6 percent of the number of updates\n",
        "    LR=2e-05                # Peak LR for polynomial LR scheduler.\n",
        "    NUM_CLASSES=2\n",
        "    MAX_SENTENCES=16        # Batch size.\n",
        "    ROBERTA_PATH=roberta.large/model.pt\n",
        "\n",
        "    CUDA_VISIBLE_DEVICES=0 fairseq-train RTE-bin/ \\\n",
        "    --restore-file $ROBERTA_PATH \\\n",
        "    --max-positions 512 \\\n",
        "    --batch-size $MAX_SENTENCES \\\n",
        "    --max-tokens 4400 \\\n",
        "    --task sentence_prediction \\\n",
        "    --reset-optimizer --reset-dataloader --reset-meters \\\n",
        "    --required-batch-size-multiple 1 \\\n",
        "    --init-token 0 --separator-token 2 \\\n",
        "    --arch roberta_large \\\n",
        "    --criterion sentence_prediction \\\n",
        "    --num-classes $NUM_CLASSES \\\n",
        "    --dropout 0.1 --attention-dropout 0.1 \\\n",
        "    --weight-decay 0.1 --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06 \\\n",
        "    --clip-norm 0.0 \\\n",
        "    --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES --warmup-updates $WARMUP_UPDATES \\\n",
        "    --fp16 --fp16-init-scale 4 --threshold-loss-scale 1 --fp16-scale-window 128 \\\n",
        "    --max-epoch 10 \\\n",
        "    --find-unused-parameters \\\n",
        "    --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric; \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvjR7TbNDOu3",
        "outputId": "d71c5154-10fe-487a-cf39-ba9d39782812"
      },
      "source": [
        "cmd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n    TOTAL_NUM_UPDATES=2036  # 10 epochs through RTE for bsz 16\\n    WARMUP_UPDATES=122      # 6 percent of the number of updates\\n    LR=2e-05                # Peak LR for polynomial LR scheduler.\\n    NUM_CLASSES=2\\n    MAX_SENTENCES=16        # Batch size.\\n    ROBERTA_PATH=roberta.large/model.pt\\n\\n    CUDA_VISIBLE_DEVICES=0 fairseq-train RTE-bin/     --restore-file $ROBERTA_PATH     --max-positions 512     --batch-size $MAX_SENTENCES     --max-tokens 4400     --task sentence_prediction     --reset-optimizer --reset-dataloader --reset-meters     --required-batch-size-multiple 1     --init-token 0 --separator-token 2     --arch roberta_large     --criterion sentence_prediction     --num-classes $NUM_CLASSES     --dropout 0.1 --attention-dropout 0.1     --weight-decay 0.1 --optimizer adam --adam-betas \"(0.9, 0.98)\" --adam-eps 1e-06     --clip-norm 0.0     --lr-scheduler polynomial_decay --lr $LR --total-num-update $TOTAL_NUM_UPDATES --warmup-updates $WARMUP_UPDATES     --fp16 --fp16-init-scale 4 --threshold-loss-scale 1 --fp16-scale-window 128     --max-epoch 10     --find-unused-parameters     --best-checkpoint-metric accuracy --maximize-best-checkpoint-metric; '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "oDXUEoAdm4Hs",
        "outputId": "5532641f-bbf3-40c6-924c-06a7f290c53f"
      },
      "source": [
        "import sys, time, csv\n",
        "import torch\n",
        "import logging\n",
        "\n",
        "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
        "\n",
        "def main():\n",
        "  models = ['tmp']\n",
        "  best_model = ''\n",
        "  accuracy = 0.0;\n",
        "  for entry in models:\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)\n",
        "    model = RobertaForMaskedLM.from_pretrained(entry)\n",
        "    # optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n",
        "    # loss_function = torch.nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "\n",
        "    # move to cuda if available\n",
        "    if torch.cuda.is_available():\n",
        "      torch.cuda.empty_cache()\n",
        "      model = model.to('cuda')\n",
        "      logging.info(\"Using cuda now\")\n",
        "\n",
        "    with open('project_dataset/test_masked.csv', newline='') as file:\n",
        "      masked_dataset = list(csv.reader(file))\n",
        "\n",
        "    with open('project_dataset/test.csv', newline='') as file:\n",
        "      original_dataset = list(csv.reader(file))\n",
        "    \n",
        "    # remove headers\n",
        "    masked_dataset.pop(0)\n",
        "    original_dataset.pop(0)\n",
        "    model_accuracy = eval(tokenizer, model, masked_dataset, original_dataset)\n",
        "    if model_accuracy > accuracy:\n",
        "      best_model = model\n",
        "      accuracy = model_accuracy\n",
        "\n",
        "max_int = sys.maxsize\n",
        "while True:\n",
        "    # decrease the maxInt value by factor 10 \n",
        "    # as long as the OverflowError occurs.\n",
        "\n",
        "    try:\n",
        "        csv.field_size_limit(max_int)\n",
        "        break\n",
        "    except OverflowError:\n",
        "        max_int = int(max_int/10)\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-9-488b1048967f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOverflowError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mmax_int\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_int\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m<ipython-input-9-488b1048967f>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobertaTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'roberta-base'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobertaClass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRobertaForMaskedLM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'RobertaClass' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhpDjB5_CVo6"
      },
      "source": [
        "# Updated accuracy based on fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ynul-XCmCVGg"
      },
      "source": [
        "from fairseq.models.roberta import RobertaModel\n",
        "\n",
        "roberta = RobertaModel.from_pretrained(\n",
        "    'checkpoints/',\n",
        "    checkpoint_file='checkpoint_best.pt',\n",
        "    data_name_or_path='RTE-bin'\n",
        ")\n",
        "\n",
        "label_fn = lambda label: roberta.task.label_dictionary.string(\n",
        "    [label + roberta.task.label_dictionary.nspecial]\n",
        ")\n",
        "ncorrect, nsamples = 0, 0\n",
        "roberta.cuda()\n",
        "roberta.eval()\n",
        "with open('glue_data/RTE/dev.tsv') as fin:\n",
        "    fin.readline()\n",
        "    for index, line in enumerate(fin):\n",
        "        tokens = line.strip().split('\\t')\n",
        "        sent1, sent2, target = tokens[1], tokens[2], tokens[3]\n",
        "        tokens = roberta.encode(sent1, sent2)\n",
        "        prediction = roberta.predict('sentence_classification_head', tokens).argmax().item()\n",
        "        prediction_label = label_fn(prediction)\n",
        "        ncorrect += int(prediction_label == target)\n",
        "        nsamples += 1\n",
        "print('| Accuracy: ', float(ncorrect)/float(nsamples))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}